package core

import (
	"fmt"
	"runtime"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
)

var runner chan func()
var runnerOnce sync.Once

func initParallelRunner(targetNum int) {
	runnerOnce.Do(func() {
		if targetNum == 0 {
			targetNum = runtime.GOMAXPROCS(0)
		}
		runner = make(chan func(), targetNum)
		for i := 0; i < targetNum; i++ {
			go func() {
				for f := range runner {
					f()
				}
			}()
		}
	})
}

func InitPevmRunner(targetNum int) {
	initParallelRunner(targetNum)
}

func ParallelNum() int {
	return cap(runner)
}

// TxLevel contains all transactions who are independent to each other
type TxLevel []*PEVMTxRequest

func (tl TxLevel) SplitBy(chunkSize int) []TxLevel {
	if len(tl) == 0 {
		return nil
	}
	if chunkSize <= 0 {
		chunkSize = 1
	}
	result := make([]TxLevel, 0, len(tl)/chunkSize+1)
	for i := 0; i < len(tl); i += chunkSize {
		end := i + chunkSize
		if end > len(tl) {
			end = len(tl)
		}
		result = append(result, tl[i:end])
	}
	return result
}

func (tl TxLevel) Split(chunks int) []TxLevel {
	if len(tl) == 0 {
		return nil
	}
	if chunks <= 0 {
		chunks = 1
	}
	result := make([]TxLevel, 0, chunks)
	var chunkSize int
	if len(tl)%chunks == 0 {
		chunkSize = len(tl) / chunks
	} else {
		chunkSize = len(tl)/chunks + 1
	}
	for i := 0; i < chunks; i++ {
		start := i * chunkSize
		end := min(start+chunkSize, len(tl))
		if start > len(tl)-1 || start >= end {
			break
		}
		result = append(result, tl[start:end])
	}
	return result
}

// TxLevels indicates the levels of transactions
// the levels are ordered by the dependencies, and generated by the TxDAG
type TxLevels []TxLevel

type confirmQueue struct {
	queue                  []confirmation
	confirmed              int // need to be set to -1 originally
	parallelMergeTime      int64
	parallelMergeAfterTime int64
}

type confirmation struct {
	result    *PEVMTxResult
	executed  error // error from execution in parallel
	confirmed error // error from confirmation in sequence (like conflict)
}

// put into the right position (txIndex)
func (cq *confirmQueue) collect(result *PEVMTxResult) error {
	if result.txReq.txIndex >= len(cq.queue) {
		// TODO add metrics
		return fmt.Errorf("txIndex outof range, req.index:%d, len(queue):%d", result.txReq.txIndex, len(cq.queue))
	}
	i := result.txReq.txIndex
	cq.queue[i].result, cq.queue[i].executed, cq.queue[i].confirmed = result, result.err, nil
	return nil
}

func (cq *confirmQueue) confirmWithUnordered(level TxLevel, execute func(*PEVMTxRequest) *PEVMTxResult, confirm func(*PEVMTxResult) error) (error, int) {
	// find all able-to-confirm transactions, and try to confirm them
	for _, tx := range level {
		i := tx.txIndex
		toConfirm := cq.queue[i]
		// the tx has not been executed yet, which means the higher-index transactions can not be confirmed before it
		// so stop the loop.
		if toConfirm.result == nil {
			break
		}
		switch true {
		case toConfirm.executed != nil:
			if err := cq.rerun(i, execute, confirm); err != nil {
				// TODO add logs for err
				// rerun failed, something very wrong.
				return err, toConfirm.result.txReq.txIndex
			}

		default:
			//try the first confirm
			if err := confirm(toConfirm.result); err != nil {
				// TODO add logs for err
				if err = cq.rerun(i, execute, confirm); err != nil {
					// TODO add logs for err
					// rerun failed, something very wrong.
					return err, toConfirm.result.txReq.txIndex
				}
			}
		}
		cq.confirmed = i
	}
	return nil, 0
}

func (cq *confirmQueue) confirmParallel(levels []TxLevel, confirm func(*PEVMTxResult) error, afterParallelConfirm func(levels TxLevels, cq *confirmQueue) (err error)) (error, int) {
	var wg sync.WaitGroup
	wg.Add(len(levels))
	errs := make(chan []interface{}, len(levels))
	start := time.Now()
	for _, txs := range levels {
		temp := txs
		run := func() {
			defer wg.Done()
			for _, tx := range temp {
				toConfirm := cq.queue[tx.txIndex]
				if toConfirm.result == nil {
					log.Warn("transaction should be executed, not result in queue", "index", tx.txIndex)
					errs <- []interface{}{fmt.Errorf("transaction should be executed, not result in queue, index %d", tx.txIndex), tx.txIndex}
					return
				}
				if toConfirm.executed != nil {
					log.Error("transaction execute fail! we can not do parallel merge here", "err", toConfirm.executed, "index", tx.txIndex)
					errs <- []interface{}{toConfirm.executed, tx.txIndex}
					return
				}
				if err := confirm(toConfirm.result); err != nil {
					log.Error("parallel merge fail!", "err", err, "index", tx.txIndex)
					errs <- []interface{}{err, tx.txIndex}
					return
				}
			}
		}
		runner <- run
	}
	go func() {
		wg.Wait()
		close(errs)
	}()
	for err := range errs {
		return err[0].(error), err[1].(int)
	}
	cq.parallelMergeTime += time.Since(start).Nanoseconds()
	start = time.Now()
	if err := afterParallelConfirm(levels, cq); err != nil {
		log.Error("confirm after parallel merge fail", "err", err)
		return err, 0
	}
	cq.parallelMergeAfterTime += time.Since(start).Nanoseconds()
	return nil, 0
}

// try to confirm txs as much as possible, they will be confirmed in a sequencial order.
func (cq *confirmQueue) confirm(execute func(*PEVMTxRequest) *PEVMTxResult, confirm func(*PEVMTxResult) error) (error, int) {
	// find all able-to-confirm transactions, and try to confirm them
	for i := cq.confirmed + 1; i < len(cq.queue); i++ {
		toConfirm := cq.queue[i]
		// the tx has not been executed yet, which means the higher-index transactions can not be confirmed before it
		// so stop the loop.
		if toConfirm.result == nil {
			break
		}
		switch true {
		case toConfirm.executed != nil:
			if err := cq.rerun(i, execute, confirm); err != nil {
				// TODO add logs for err
				// rerun failed, something very wrong.
				return err, toConfirm.result.txReq.txIndex
			}

		default:
			//try the first confirm
			if err := confirm(toConfirm.result); err != nil {
				// TODO add logs for err
				if err = cq.rerun(i, execute, confirm); err != nil {
					// TODO add logs for err
					// rerun failed, something very wrong.
					return err, toConfirm.result.txReq.txIndex
				}
			}
		}
		cq.confirmed = i
	}
	return nil, 0
}

// rerun executes the transaction of index 'i', and confirms it.
func (cq *confirmQueue) rerun(i int, execute func(*PEVMTxRequest) *PEVMTxResult, confirm func(*PEVMTxResult) error) error {
	//reset the result
	cq.queue[i].result.err, cq.queue[i].executed, cq.queue[i].confirmed = nil, nil, nil
	// failed, rerun and reconfirm, the rerun should alway success.
	rerun := execute(cq.queue[i].result.txReq)
	if rerun.err != nil {
		// TODO add metrics, add error logs.
		return rerun.err
	}
	cq.queue[i].result, cq.queue[i].executed, cq.queue[i].confirmed = rerun, nil, confirm(rerun)
	if cq.queue[i].confirmed != nil {
		// TODO add metrics, add error logs.
		return cq.queue[i].confirmed
	}
	return nil
}

var goMaxProcs = runtime.GOMAXPROCS(0)

// run runs the transactions in parallel
// execute must return a non-nil result, otherwise it panics.
func (tls TxLevels) Run(execute func(*PEVMTxRequest) *PEVMTxResult, confirm func(*PEVMTxResult) error, afterParallelConfirm func(levels TxLevels, cq *confirmQueue) (err error), unorderedMerge bool, parallelMerge bool) (error, int) {
	toConfirm := &confirmQueue{
		queue:     make([]confirmation, tls.txCount()),
		confirmed: -1,
	}

	totalExecutionTime := int64(0)
	totalConfirmTime := int64(0)
	maxLevelTxCount := 0

	// execute all transactions in parallel
	for _, txLevel := range tls {
		start := time.Now()
		log.Debug("txLevel tx count", "tx count", len(txLevel))
		if len(txLevel) > maxLevelTxCount {
			maxLevelTxCount = len(txLevel)
		}
		wait := sync.WaitGroup{}
		trunks := txLevel.Split(goMaxProcs)
		wait.Add(len(trunks))
		// split tx into chunks, to save the cost of channel communication
		for _, txs := range trunks {
			// execute the transactions in parallel
			temp := txs
			run := func() {
				for _, tx := range temp {
					res := execute(tx)
					toConfirm.collect(res)
				}
				wait.Done()
			}
			//go run()
			runner <- run
		}
		wait.Wait()
		totalExecutionTime += time.Since(start).Nanoseconds()
		start = time.Now()
		// all transactions of current level are executed, now try to confirm.
		if parallelMerge {
			if err, txIndex := toConfirm.confirmParallel(trunks, confirm, afterParallelConfirm); err != nil {
				return err, txIndex
			}
		} else if unorderedMerge {
			if err, txIndex := toConfirm.confirmWithUnordered(txLevel, execute, confirm); err != nil {
				// something very wrong, stop the process
				return err, txIndex
			}
		} else {
			if err, txIndex := toConfirm.confirm(execute, confirm); err != nil {
				// something very wrong, stop the process
				return err, txIndex
			}
		}
		totalConfirmTime += time.Since(start).Nanoseconds()
	}
	parallelTxLevelTxSizeMeter.Update(int64(maxLevelTxCount))
	parallelExecutionTimer.Update(time.Duration(totalExecutionTime))
	parallelConfirmTimer.Update(time.Duration(totalConfirmTime))
	parallelConfirmConcurrentTimer.Update(time.Duration(toConfirm.parallelMergeTime))
	parallelConfirmAfterTimer.Update(time.Duration(toConfirm.parallelMergeAfterTime))
	return nil, 0
}

func (tls TxLevels) txCount() int {
	count := 0
	for _, txlevel := range tls {
		count += len(txlevel)
	}
	return count
}

// predictTxDAG predicts the TxDAG by their from address and to address, and generates the levels of transactions
func (tl TxLevel) predictTxDAG(dag types.TxDAG) {
	marked := make(map[common.Address]int, len(tl))
	for _, tx := range tl {
		var deps []uint64
		var tfrom, tto = -1, -1
		if ti, ok := marked[tx.msg.From]; ok {
			tfrom = ti
		}
		if ti, ok := marked[*tx.msg.To]; ok {
			tto = ti
		}
		if tfrom >= 0 && tto >= 0 && tfrom > tto {
			// keep deps ordered by the txIndex
			tfrom, tto = tto, tfrom
		}
		if tfrom >= 0 {
			deps = append(deps, uint64(tfrom))
		}
		if tto >= 0 {
			deps = append(deps, uint64(tto))
		}
		dag.SetTxDep(tx.txIndex, types.TxDep{TxIndexes: deps})
		marked[tx.msg.From] = tx.txIndex
		marked[*tx.msg.To] = tx.txIndex
	}
}

func NewTxLevels(all []*PEVMTxRequest, dag types.TxDAG) TxLevels {
	var levels TxLevels = make(TxLevels, 0, 8)
	var enlargeLevelsIfNeeded = func(currLevel int, levels *TxLevels) {
		if len(*levels) <= currLevel {
			for i := len(*levels); i <= currLevel; i++ {
				*levels = append(*levels, TxLevel{})
			}
		}
	}

	if len(all) == 0 {
		return nil
	}
	if dag == nil {
		return TxLevels{all}
	}

	// build the levels from the DAG
	marked, _ := BuildTxLevels(len(all), dag)
	// put the transactions into the levels
	for txIndex, tx := range all {
		level := marked[txIndex]
		enlargeLevelsIfNeeded(level, &levels)
		levels[level] = append(levels[level], tx)
	}
	return levels
}

func BuildTxLevels(txCount int, dag types.TxDAG) (marked map[int]int, depth int) {
	if dag == nil {
		return make(map[int]int), 0
	}
	// marked is used to record which level that each transaction should be put
	marked = make(map[int]int, txCount)
	var (
		// currLevelHasTx marks if the current level has any transaction
		currLevelHasTx bool
	)

	depth, currLevelHasTx = 0, false
	for txIndex := 0; txIndex < txCount; txIndex++ {
		dep := dag.TxDep(txIndex)
		switch true {
		case dep != nil && dep.CheckFlag(types.ExcludedTxFlag),
			dep != nil && dep.CheckFlag(types.NonDependentRelFlag):
			// excluted tx, occupies the whole level
			// or dependent-to-all tx, occupies the whole level, too
			if currLevelHasTx {
				// shift to next level if there are transactions in the current level
				depth++
			}
			marked[txIndex] = depth
			// occupy the current level
			depth, currLevelHasTx = depth+1, false

		case dep == nil || len(dep.TxIndexes) == 0:
			// dependent on none, just put it in the current level
			marked[txIndex], currLevelHasTx = depth, true

		case dep != nil && len(dep.TxIndexes) > 0:
			// dependent on others
			// findout the correct level that the tx should be put
			prevLevel := -1
			for _, txIndex := range dep.TxIndexes {
				if pl, ok := marked[int(txIndex)]; ok && pl > prevLevel {
					prevLevel = pl
				}
			}
			if prevLevel < 0 {
				// broken DAG, just ignored it
				marked[txIndex], currLevelHasTx = depth, true
				continue
			}
			// record the level of this tx
			marked[txIndex] = prevLevel + 1
			if marked[txIndex] > depth {
				depth, currLevelHasTx = marked[txIndex], true
			}

		default:
			panic("unexpected case")
		}
	}
	// check if the last level has any transaction, to avoid the empty level
	if !currLevelHasTx {
		depth--
	}
	return marked, depth + 1
}
